[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /opt/airflow/dags

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = CeleryExecutor

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The location of the key file
fernet_key =

# Whether to load the examples that ship with airflow. It's good to
# get started, but you probably want to set this to false in a
# production environment
load_examples = True

# Where your log files will be stored
base_log_folder = /opt/airflow/logs

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

[webserver]
# The base url of your web application
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Number of seconds the gunicorn webserver waits before timing out on a request
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is disabled. When nonzero, airflow periodically refreshes webserver workers by bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 30

[celery]
# The app name to use for running Celery
celery_app_name = airflow.executors.celery_executor

# The broker URL to use for Celery
broker_url = redis://redis-celery:6379/0

# The backend URL to use for storing Celery task results
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# The configuration options for Celery
flower_port = 5555
default_queue = default

[scheduler]
# The scheduler is responsible for kicking off scheduled tasks, it needs to be running for scheduled tasks to be executed
# The number of seconds to wait between polling the executor for changes
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5

# How often (in seconds) to check and update task instance state
scheduler_task_queued_timeout = 300

# How often should stats be printed to the logs
print_stats_interval = 30

# Number of seconds to wait before refreshing a batch of workers
child_process_log_directory = /opt/airflow/logs/scheduler

[logging]
# The logging level
logging_level = INFO

# The directory for the logs
base_log_folder = /opt/airflow/logs
processor_log_folder = /opt/airflow/logs/scheduler

# Log format for tasks
task_log_format = [%%(asctime)s] {%%(process)d} %%(
